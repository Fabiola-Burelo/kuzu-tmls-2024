{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphQA using LangChain and Kùzu\n",
    "In this notebook, we will use the LangChain library to answer questions on top of the Kùzu graph we\n",
    "just created in the previous section. The example below uses the OpenAI GPT-3.5 turbo model to generate\n",
    "Cypher and answer questions via a text-to-Cypher pipeline, but you can use any other model and see\n",
    "how it performs.\n",
    "\n",
    "We start by opening a connection to the existing database and loading the `OPENAI_API_KEY` variable\n",
    "from a local `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !uv pip install python-dotenv langchain langchain-community langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import kuzu\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load OpenAI API key from .env file\n",
    "load_dotenv()\n",
    "assert \"OPENAI_API_KEY\" in os.environ, \"Please set OPENAI_API_KEY in the .env file\"\n",
    "\n",
    "db = kuzu.Database(\"../ex_db_kuzu\")\n",
    "conn = kuzu.Connection(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import KuzuQAChain\n",
    "from langchain_community.graphs import KuzuGraph\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node properties: [{'properties': [('id', 'INT64'), ('account_id', 'STRING'), ('balance', 'DOUBLE'), ('betweenness_centrality', 'DOUBLE')], 'label': 'Account'}, {'properties': [('address', 'STRING')], 'label': 'Address'}, {'properties': [('id', 'INT64'), ('name', 'STRING'), ('state', 'STRING'), ('zip', 'INT64'), ('email', 'STRING')], 'label': 'Person'}]\n",
      "Relationships properties: [{'properties': [], 'label': 'Owns'}, {'properties': [], 'label': 'LivesIn'}, {'properties': [('amount', 'DOUBLE')], 'label': 'Transfer'}]\n",
      "Relationships: ['(:Person)-[:Owns]->(:Account)', '(:Person)-[:LivesIn]->(:Address)', '(:Account)-[:Transfer]->(:Account)']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a graph object for KuzuQAChain and print the schema\n",
    "graph = KuzuGraph(db)\n",
    "print(graph.get_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This schema is passed as part of the prompt to the LLM, which is then used to generate the Cypher query.\n",
    "The following example shows how to the GPT-3.5 turbo model is used for both text-to-Cypher and for\n",
    "answer generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = KuzuQAChain.from_llm(\n",
    "    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, api_key=os.environ.get(\"OPENAI_API_KEY\")),\n",
    "    graph=graph,\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then ask questions in natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new KuzuQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH ()-[:Transfer {amount: $highest_amount}]->()\n",
      "RETURN $highest_amount\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'What is the highest amount transferred in this dataset?',\n",
       " 'result': \"I don't know the answer.\"}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What is the highest amount transferred in this dataset?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new KuzuQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH path = (:Person {name: 'Karissa'})-[*1..3]-(:Person {name: 'Anna'})\n",
      "RETURN path\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'path': {'_nodes': [{'_id': {'offset': 6, 'table': 0}, '_label': 'Person', 'id': 7, 'name': 'Karissa', 'state': 'FL', 'zip': 32325, 'email': 'kpnichols_malone@gmail.com', 'address': None, 'account_id': None, 'balance': None, 'betweenness_centrality': None}, {'_id': {'offset': 6, 'table': 2}, '_label': 'Account', 'id': 7, 'name': None, 'state': None, 'zip': None, 'email': None, 'address': None, 'account_id': '120747146', 'balance': 7354.0, 'betweenness_centrality': 0.0}, {'_id': {'offset': 14, 'table': 2}, '_label': 'Account', 'id': 15, 'name': None, 'state': None, 'zip': None, 'email': None, 'address': None, 'account_id': '073343503', 'balance': 8048.0, 'betweenness_centrality': 0.006535947712418301}, {'_id': {'offset': 14, 'table': 0}, '_label': 'Person', 'id': 15, 'name': 'Anna', 'state': 'MN', 'zip': 55084, 'email': 'amrowland@gmail.com', 'address': None, 'account_id': None, 'balance': None, 'betweenness_centrality': None}], '_rels': [{'_src': {'offset': 6, 'table': 0}, '_dst': {'offset': 6, 'table': 2}, '_label': 'Owns', 'amount': None}, {'_src': {'offset': 6, 'table': 2}, '_dst': {'offset': 14, 'table': 2}, '_label': 'Transfer', 'amount': 76.0}, {'_src': {'offset': 14, 'table': 0}, '_dst': {'offset': 14, 'table': 2}, '_label': 'Owns', 'amount': None}]}}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'Is there a path up to a depth of 3 between the persons named Karissa and Anna?',\n",
       " 'result': 'Yes, there is a path up to a depth of 3 between the persons named Karissa and Anna.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Is there a path up to a depth of 3 between the persons named Karissa and Anna?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with open source LLMs\n",
    "You can use different LLMs, including open source ones, for text-to-Cypher and the answer generation stages. See the\n",
    "[LangChain docs](https://python.langchain.com/v0.2/docs/integrations/graphs/kuzu_db/#use-separate-llms-for-cypher-and-answer-generation)\n",
    "for such an example.\n",
    "\n",
    "Open source LLMs can be self-hosted and served on a local endpoint. In\n",
    "this example, we use a _much_ cheaper locally running `Mistral-7B-OpenOrca-GGUF` model from LMStudio\n",
    "for text-to-Cypher, followed by OpenAI's GPT-3.5 turbo model for answer generation. We are still able\n",
    "to call the `ChatOpenAI` class in both cases because LMStudio's local server mimics OpenAI's API endpoints.\n",
    "\n",
    "Note that cheap and small open source LLMs may not perform as well as the proprietary, general-purpose ones,\n",
    "so to obtain best performance on Cypher generation (as well as inference from Cypher quuery results),\n",
    "you may need to fine-tune a more powerful model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = KuzuQAChain.from_llm(\n",
    "    qa_llm=ChatOpenAI(base_url=\"http://localhost:1234/v1\", temperature=0, api_key=\"not_needed\"),\n",
    "    cypher_llm=ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0, api_key=os.environ.get(\"OPENAI_API_KEY\")),\n",
    "    graph=graph,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new KuzuQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (p:Person)-[:LivesIn]->(a:Address)\n",
      "RETURN a, COUNT(p) AS numPeople\n",
      "ORDER BY numPeople DESC\n",
      "LIMIT 3\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'a': {'_id': {'offset': 1, 'table': 1}, '_label': 'Address', 'address': '953 Kyle Loop, Johnview, MT 59065'}, 'numPeople': 3}, {'a': {'_id': {'offset': 2, 'table': 1}, '_label': 'Address', 'address': '1919 Joshua Square Suite 643, Carmen Town, WY 83111'}, 'numPeople': 2}, {'a': {'_id': {'offset': 4, 'table': 1}, '_label': 'Address', 'address': '829 Zimmerman Points Suite 688, Port Eduardo, FL 32325'}, 'numPeople': 2}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'Which addresses have the most people living in them? Return only the top 3.',\n",
       " 'result': ' The address with the most people living in it is \"953 Kyle Loop, Johnview, MT 59065\" with 3 people. The second address with the most people is \"1919 Joshua Square Suite 643, Carmen Town, WY 83111\" with 2 people. The third address with the most people is \"829 Zimmerman Points Suite 688, Port Eduardo, FL 32325\" also with 2 people.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Which addresses have the most people living in them? Return only the top 3.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "Feel free to experiment with other LLMs and see how they perform on your own data. As the natural\n",
    "language questions become more complex, it might result in incorrect Cypher generation, no matter\n",
    "how good the underlying LLM. In such cases, a query rewriting step may be required to provide better\n",
    "context to the cypher-generating LLM.\n",
    "\n",
    "This notebook is just the starting point of utilizing knowledge graphs for retrieval and QA tasks. You can\n",
    "look at more advanced pipelines that utilize agents, memory and routers via the LangChain and LlamaIndex frameworks,\n",
    "or simply roll out your own abstractions that fit your use case.\n",
    "\n",
    "Have fun using graphs, and `pip install kuzu`!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmls",
   "language": "python",
   "name": "tmls"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
